


{
  "IsNullWithPreCond": {
    "json_template": {
      "_comment": "{rule_id}|{table}|{column}",
      "name": "satisfies",
      "args": {
        "column": "CAST({column} AS decimal(38,18)) is null"
      },
      "condition": "== 1.0",
      "where_cond": "{where_condition}"
    },
    "placeholders": ["rule_id", "table", "column", "where_condition"]
  },
  "LTECheckWithPreCond": {
    "json_template": {
      "_comment": "{rule_id}|{table}|{column}",
      "name": "satisfies",
      "args": {
        "column": "unix_timestamp({column},'ddMMMyyyy') <= unix_timestamp({compare_column},'ddMMMyyyy')"
      },
      "condition": "== 1.0",
      "where_cond": "{where_condition}"
    },
    "placeholders": ["rule_id", "table", "column", "compare_column", "where_condition"]
  }
}



import re
import json
import pandas as pd
from collections import defaultdict
import streamlit as st
from io import BytesIO

PATTERN_FILE = "patterns.json"


# ---------------- Rule Parser ----------------
class RuleParser:
    def __init__(self, pattern_file=PATTERN_FILE):
        try:
            with open(pattern_file, "r") as f:
                self.patterns = json.load(f)
        except FileNotFoundError:
            self.patterns = {}
        self.pattern_file = pattern_file

    def save_patterns(self):
        with open(self.pattern_file, "w") as f:
            json.dump(self.patterns, f, indent=2)

    def get_func_name(self, rule):
        func = re.match(r"(\w+)\(", rule)
        return func.group(1) if func else None

    def parse_rule(self, rule, rule_id):
        func_name = self.get_func_name(rule)
        if not func_name:
            return {"error": "Invalid rule format"}

        if func_name not in self.patterns:
            return {"unknown_pattern": func_name, "rule": rule}

        # Use known template
        pattern = self.patterns[func_name]
        json_template = pattern["json_template"]

        args = re.findall(r"<(.*?)>", rule)
        conds = re.findall(r"\{(.*?)\}", rule)

        data = {
            "rule_id": rule_id,
            "rule": rule,
            "table": args[0].split(".")[0] if args else "",
            "column": args[0].split(".")[-1] if args else "",
            "compare_column": args[1].split(".")[-1] if len(args) > 1 else "",
            "min_value": args[1] if len(args) > 1 else "",
            "max_value": args[2] if len(args) > 2 else "",
            "expected_value": args[1] if len(args) > 1 else "",
            "where_condition": conds[0] if conds else ""
        }

        parsed_json = json.loads(json.dumps(json_template).format(**data))
        return parsed_json


# ---------------- Streamlit UI ----------------
st.title("üß† Self-Learning DSL ‚Üí Deequ JSON Trainer (Cluster Mode)")

uploaded_file = st.file_uploader("Upload Excel file with 'input rule' column", type=["xlsx"])

if uploaded_file:
    df = pd.read_excel(uploaded_file)
    parser = RuleParser()

    # Cluster unknowns
    unknown_clusters = defaultdict(list)
    parsed_results = []

    for idx, row in df.iterrows():
        rule = str(row["input rule"])
        parsed = parser.parse_rule(rule, idx + 1)

        if "unknown_pattern" in parsed:
            unknown_clusters[parsed["unknown_pattern"]].append(rule)
            parsed_results.append("{}")  # placeholder
        else:
            parsed_results.append(json.dumps(parsed))

    # Train unknown patterns
    if unknown_clusters:
        st.subheader("‚ö† Unknown Patterns Found")
        for func_name, rules in unknown_clusters.items():
            st.warning(f"Pattern `{func_name}` not defined. {len(rules)} rules need it.")
            st.text(f"Example: {rules[0]}")

            json_template_str = st.text_area(
                f"Enter JSON template for `{func_name}` (use placeholders like {{column}}, {{where_condition}})",
                height=200,
                key=func_name
            )

            if st.button(f"Save Pattern `{func_name}`"):
                try:
                    parser.patterns[func_name] = {
                        "json_template": json.loads(json_template_str),
                        "placeholders": []
                    }
                    parser.save_patterns()
                    st.success(f"‚úÖ Pattern `{func_name}` saved! Re-run parsing.")
                except Exception as e:
                    st.error(f"Invalid JSON template: {e}")

    # Write output Excel with placeholder JSON
    df["aws json parsed rule"] = parsed_results
    buffer = BytesIO()
    df.to_excel(buffer, index=False)
    buffer.seek(0)

    st.download_button(
        label="üíæ Download Processed Excel",
        data=buffer,
        file_name="rules_with_json.xlsx",
        mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
    )

-----------
import re
import json
from fuzzywuzzy import fuzz

# -----------------------------
# 1Ô∏è‚É£ Historical patterns
# -----------------------------
history_path = "dq_history.json"
try:
    with open(history_path, "r") as f:
        history = json.load(f)
except:
    history = []

# -----------------------------
# 2Ô∏è‚É£ Parse single condition
# -----------------------------
def parse_condition(cond: str) -> str:
    cond = cond.strip()
    patterns = [
        (r"IsEqual\((\w+)\s*=\s*([^)]+)\)", lambda m: f"{m.group(1)} = '{m.group(2).strip()}'"),
        (r"IsIn\((\w+);\s*\{([^}]+)\}\)", lambda m: f"{m.group(1)} IN ({', '.join([f\"'{v.strip()}'\" for v in m.group(2).split(',')])})"),
        (r"IsGreaterThanOrEqual\((\w+);\s*([^)]+)\)", lambda m: f"{m.group(1)} >= {m.group(2).strip()}"),
        (r"IsLessThanOrEqual\((\w+);\s*([^)]+)\)", lambda m: f"{m.group(1)} <= {m.group(2).strip()}"),
        (r"NotInRange\((\w+);\s*([^;]+);\s*([^)]+)\)", lambda m: f"NOT({m.group(1)} BETWEEN {m.group(2).strip()} AND {m.group(3).strip()})")
    ]
    for pattern, func in patterns:
        m = re.match(pattern, cond, re.IGNORECASE)
        if m: return func(m)
    return None

# -----------------------------
# 3Ô∏è‚É£ Parse nested AND/OR
# -----------------------------
def parse_logical_expression(expr: str) -> str:
    expr = expr.strip()
    while expr.startswith("(") and expr.endswith(")"): expr = expr[1:-1].strip()
    or_parts = [p.strip() for p in re.split(r"\bOR\b", expr, flags=re.IGNORECASE)]
    parsed_or = []
    for part in or_parts:
        and_parts = [parse_condition(p.strip()) if "(" not in p else parse_logical_expression(p.strip()) 
                     for p in re.split(r"\bAND\b", part, flags=re.IGNORECASE)]
        parsed_or.append(" AND ".join(and_parts))
    return " OR ".join(f"({p})" if " AND " in p else p for p in parsed_or)

# -----------------------------
# 4Ô∏è‚É£ Nearest historical pattern
# -----------------------------
def nearest_historical_pattern(dq_expr: str, threshold=60):
    func_name_match = re.match(r"(\w+)\(", dq_expr.strip())
    if not func_name_match: return None
    func_name = func_name_match.group(1)
    best_match = None
    best_score = 0
    for entry in history:
        score = fuzz.ratio(func_name.lower(), entry["pattern_name"].lower())
        if score > best_score:
            best_score = score
            best_match = entry
    if best_score >= threshold:
        return best_match, best_score
    return None, best_score

# -----------------------------
# 5Ô∏è‚É£ Self-learning parser
# -----------------------------
def self_learning_parser(dq_expr: str):
    match, score = nearest_historical_pattern(dq_expr)
    if match:
        func_type = match["pattern_name"]
        constraint = match.get("constraint", "compliance")
    else:
        # Unknown pattern ‚Üí add to history
        func_type = re.match(r"(\w+)\(", dq_expr.strip()).group(1)
        constraint = "compliance"
        history.append({
            "pattern_name": func_type,
            "example_rule": dq_expr,
            "constraint": constraint
        })
    
    # Handle IsNullPrecondition
    if "IsNullPrecondition" in func_type:
        m = re.match(r"IsNullPrecondition\(\((\w+)\);\s*(.+)\)$", dq_expr.strip(), re.IGNORECASE)
        if not m: raise ValueError("Cannot parse IsNullPrecondition")
        attr1, cond_expr = m.groups()
        where_clause = parse_logical_expression(cond_expr)
        return {
            "constraint": constraint,
            "column": attr1,
            "where": where_clause,
            "assertion": ">= 1.0",
            "level": "Error",
            "description": f"Ensure {attr1} is not NULL whenever {where_clause}"
        }
    else:
        rule = parse_condition(dq_expr)
        if not rule: rule = dq_expr
        return {
            "constraint": constraint,
            "column": "custom_case_rule",
            "rule": rule,
            "assertion": ">= 1.0",
            "level": "Error",
            "description": f"Check condition: {rule}"
        }

# -----------------------------
# 6Ô∏è‚É£ Save history
# -----------------------------
def save_history():
    with open(history_path, "w") as f:
        json.dump(history, f, indent=2)




-----2
import streamlit as st
import pandas as pd
import json
from dq_parser import self_learning_parser, history, save_history

st.title("Self-Learning DQ Rule ‚Üí AWS Deequ JSON Converter üöÄ")

# Upload file
uploaded_file = st.file_uploader("Upload Excel or Hive SQL", type=["xlsx", "xls", "sql"])
rules_list = []

if uploaded_file:
    if uploaded_file.name.endswith(".sql"):
        lines = uploaded_file.read().decode("utf-8").splitlines()
        rules_list = [{"rule_id": i+1, "dq_function": line.strip()} for i, line in enumerate(lines) if line.strip()]
    else:
        df = pd.read_excel(uploaded_file)
        if "dq_function" not in df.columns:
            st.error("Excel must have column 'dq_function'")
        else:
            rules_list = df.to_dict(orient="records")

# Parse rules
if rules_list:
    st.subheader("Parsed Deequ JSON")
    deequ_json_list = []
    failed_rules = []

    for rule in rules_list:
        dq_func = rule["dq_function"]
        try:
            deequ_json = self_learning_parser(dq_func)
            deequ_json["rule_id"] = rule["rule_id"]
            deequ_json_list.append(deequ_json)
        except Exception as e:
            failed_rules.append({"rule_id": rule["rule_id"], "dq_function": dq_func, "error": str(e)})

    st.json(deequ_json_list)

    if failed_rules:
        st.warning(f"{len(failed_rules)} rules failed parsing")
        st.write(failed_rules)

# Show historical patterns
st.subheader("Historical Patterns Knowledge Base")
st.write(history)

# Save updated patterns
if st.button("Save Updated Patterns"):
    save_history()
    st.success("Historical patterns saved successfully!")



from pptx import Presentation
from pptx.util import Inches, Pt
from pptx.dml.color import RGBColor
from pptx.enum.shapes import MSO_SHAPE
from pptx.oxml.xmlchemy import OxmlElement

# Helper: add text box
def add_textbox(slide, left, top, width, height, text, font_size=14, bold=False, color=RGBColor(0,0,0), fill_color=None):
    shape = slide.shapes.add_shape(MSO_SHAPE.RECTANGLE, left, top, width, height)
    shape.text = text
    shape.text_frame.paragraphs[0].font.size = Pt(font_size)
    shape.text_frame.paragraphs[0].font.bold = bold
    shape.text_frame.paragraphs[0].font.color.rgb = color
    if fill_color:
        fill = shape.fill
        fill.solid()
        fill.fore_color.rgb = fill_color
    return shape

# Helper: add dashed connector
def add_dashed_connector(slide, shape1, shape2):
    connector = slide.shapes.add_connector(
        begin_x=shape1.left + shape1.width,
        begin_y=shape1.top + shape1.height / 2,
        end_x=shape2.left,
        end_y=shape2.top + shape2.height / 2
    )
    ln = connector.line
    ln.color.rgb = RGBColor(255, 0, 0)
    ln.width = Pt(2)
    ln.dash_style = "sysDot"

# Create presentation
prs = Presentation()
slide = prs.slides.add_slide(prs.slide_layouts[6])  # blank layout

# Colors
blue = RGBColor(0, 153, 204)
light_blue = RGBColor(173, 216, 230)

# Main "DQ Editor" block
dq_editor = add_textbox(slide, Inches(1), Inches(1.5), Inches(8), Inches(4), "DQ Editor", font_size=16, bold=True, fill_color=blue)

# Sub-boxes inside DQ Editor
lob = add_textbox(slide, Inches(2), Inches(2.2), Inches(1.5), Inches(1), "LOB", fill_color=light_blue)
dataset = add_textbox(slide, Inches(3.8), Inches(2.2), Inches(2), Inches(1), "Dataset\nPhysical-Logical", fill_color=light_blue)
s3sql = add_textbox(slide, Inches(6.2), Inches(2.2), Inches(2.2), Inches(1.2), "S3 Rules JSON Dir\nSQL Dir", fill_color=light_blue)

# Left side boxes
pre_reg = add_textbox(slide, Inches(0.5), Inches(2), Inches(0.8), Inches(1.5), "Pre-\nRegistration", fill_color=light_blue)
rules_onboard = add_textbox(slide, Inches(0.5), Inches(3.8), Inches(0.8), Inches(1.2), "Rules\nOnboarding", fill_color=light_blue)

# Lower row
dqip = add_textbox(slide, Inches(2.5), Inches(4.1), Inches(2), Inches(1), "DQIP template\nFile staging\n(New Rules)", fill_color=light_blue)
pre_existing = add_textbox(slide, Inches(5.2), Inches(4.1), Inches(2.5), Inches(1), "Pre-existing Rules\nUpdate via UI", fill_color=light_blue)

# Top box (Data Catalogue)
data_catalogue = add_textbox(slide, Inches(3.5), Inches(0.6), Inches(3), Inches(0.8), "Data Catalogue", font_size=16, bold=True, fill_color=light_blue)

# Add dashed arrow
add_dashed_connector(slide, dqip, pre_existing)

# Save
prs.save("DQ_Architecture.pptx")
print("‚úÖ Generated 'DQ_Architecture.pptx'")


